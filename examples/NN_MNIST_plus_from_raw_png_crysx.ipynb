{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb703b4",
   "metadata": {},
   "source": [
    "# This is a demo of how to load MNIST raw pngs manually and use CrysX for neural network machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c925c8",
   "metadata": {},
   "source": [
    "## Run the following for Google colab \n",
    "then restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --no-cache-dir https://github.com/manassharma07/crysx_nn/tarball/main\n",
    "! pip install IPython==7.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from crysx_nn import mnist_utils as mu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2973a31",
   "metadata": {},
   "source": [
    "## Download MNIST_orig and MNIST_plus dataset (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afde5d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mu.downloadMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7de5a",
   "metadata": {},
   "source": [
    "## Load the training dataset from MNIST_plus in memory (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "trainData, trainLabels = mu.loadMNIST(path_main=path, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b75697",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape', trainData.shape)\n",
    "print('Training labels shape',trainLabels.shape)\n",
    "print('Size of training data in memory (GB)', trainData.nbytes/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 255.\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(trainData.std()) # Expected for MNIST_orig: 78.567489983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637000b",
   "metadata": {},
   "source": [
    "## Normalize within the range [0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData/255 # Normalize\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(trainData.std()) # Expected for MNIST_orig: 0.3081078038564622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed0fd",
   "metadata": {},
   "source": [
    "## Standardize the data so that it has mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = (trainData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(trainData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.std()) # Expected for MNIST_orig: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1b0af",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1796640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainLabels)\n",
    "trainLabels = mu.one_hot_encode(trainLabels, 10)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c8407",
   "metadata": {},
   "source": [
    "## Flatten the input numpy arrays (nSamples,28,28)->(nSamples, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70109dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData.reshape(trainData.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f82965",
   "metadata": {},
   "source": [
    "## Let us create a NN using CrysX-NN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756390d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nInputs = 784 # No. of nodes in the input layer\n",
    "neurons_per_layer = [256, 10] # Neurons per layer (excluding the input layer)\n",
    "activation_func_names = ['ReLU', 'Softmax']\n",
    "nLayers = len(neurons_per_layer)\n",
    "nEpochs=15\n",
    "batchSize = 32 # No. of input samples to process at a time for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80740bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crysx_nn import network\n",
    "model = network.nn_model(nInputs=nInputs, neurons_per_layer=neurons_per_layer, activation_func_names=activation_func_names, batch_size=batchSize, device='CPU', init_method='Xavier') \n",
    "\n",
    "model.lr = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f95885",
   "metadata": {},
   "source": [
    "## Check the model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d70f66",
   "metadata": {},
   "source": [
    "## Optimize/Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6931984",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainData.astype(np.float32)\n",
    "outputs = trainLabels.astype(np.float32)\n",
    "# Run optimization\n",
    "model.optimize(inputs, outputs, lr=0.4,nEpochs=nEpochs,loss_func_name='CCE', miniterEpoch=1, batchProgressBar=True, miniterBatch=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6156fb",
   "metadata": {},
   "source": [
    "## Error at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7041f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crysx_nn import loss\n",
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = np.zeros((1, 10), dtype=np.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = np.full_like(labels, 1./N)\n",
    "print(loss.BCE_loss(output, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff259",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "testData, testLabels = mu.loadMNIST(path_main=path, train=False, shuffle=True)\n",
    "\n",
    "print('Test data shape', testData.shape)\n",
    "print('Test labels shape',testLabels.shape)\n",
    "print('Size of training data in memory (GB)', testData.nbytes/1024/1024/1024)\n",
    "\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 255.\n",
    "print(testData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(testData.std()) # Expected for MNIST_orig: 78.567489983\n",
    "\n",
    "## Normalize within the range [0,1.0]\n",
    "\n",
    "testData = testData/255 # Normalize\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(testData.std()) # Expected for MNIST_orig: 0.3081078038564622 \n",
    "\n",
    "## Standardize the data so that it has mean 0 and variance 1\n",
    "# Use the mean and std of training data **********\n",
    "testData = (testData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(testData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.std()) # Expected for MNIST_orig: 1.0000\n",
    "\n",
    "## Convert labels to one-hot vectors\n",
    "print(testLabels)\n",
    "testLabels = mu.one_hot_encode(testLabels, 10)\n",
    "print(testLabels)\n",
    "\n",
    "## Flatten the input numpy arrays (nSamples,28,28)->(nSamples, 784)\n",
    "testData = testData.reshape(testData.shape[0], 784)\n",
    "print(testData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abe86",
   "metadata": {},
   "source": [
    "## Performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e291aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Convert to float32 arrays\n",
    "inputs = testData.astype(np.float32)\n",
    "outputs = testLabels.astype(np.float32)\n",
    "predictions, error = model.predict(inputs, outputs, loss_func_name='BCE')\n",
    "print(error)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb79a7",
   "metadata": {},
   "source": [
    "## Interactive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "974f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.71775480e-02 7.48407476e-02 4.22317087e-01 1.46121089e-01\n",
      "  1.40692068e-04 2.27736304e-01 2.49687124e-03 1.05235908e-01\n",
      "  1.22463296e-03 2.70912070e-03]]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *\n",
    "import cv2\n",
    "\n",
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "white = (255, 255, 255) # canvas back\n",
    "\n",
    "def save():\n",
    "    # save image to hard drive\n",
    "    filename = \"user_input.jpg\"\n",
    "    global output_image \n",
    "    output_image.save(filename)\n",
    "    ###### Centering begin\n",
    "    # Load image as grayscale and obtain bounding box coordinates\n",
    "    image = cv2.imread('user_input.jpg', 0)\n",
    "#     print(image)\n",
    "    height, width = image.shape\n",
    "    x,y,w,h = cv2.boundingRect(image)\n",
    "\n",
    "    # Create new blank image and shift ROI to new coordinates\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    mask = np.zeros([ROI.shape[0]+10,ROI.shape[1]+10])\n",
    "    width, height = mask.shape\n",
    "#     print(ROI.shape)\n",
    "#     print(mask.shape)\n",
    "    x = width//2 - ROI.shape[0]//2 \n",
    "    y = height//2 - ROI.shape[1]//2 \n",
    "#     print(x,y)\n",
    "    mask[y:y+h, x:x+w] = ROI\n",
    "#     print(mask)\n",
    "    # Check if centering/masking was successful\n",
    "#     plt.imshow(mask, cmap='viridis') \n",
    "    output_image = PIL.Image.fromarray(mask) # mask has values in [0-255] as expected\n",
    "# Now we need to resize, but it causes problems with default arguments as it changes the range of pixel values to be negative or positive\n",
    "    # compressed_output_image = output_image.resize((22,22))\n",
    "    # Therefore, we use the following:\n",
    "    compressed_output_image = output_image.resize((22,22), PIL.Image.BILINEAR) # PIL.Image.NEAREST or PIL.Image.BILINEAR also performs good\n",
    "\n",
    "#     # Enhance Saturation\n",
    "#     converter = PIL.ImageEnhance.Color(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(2.5)\n",
    "    # Enhance contrast\n",
    "#     converter = PIL.ImageEnhance.Contrast(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(3.5)\n",
    "    # normalize in the range 0-1\n",
    "    tensor_image = np.array(compressed_output_image.getdata())/255.\n",
    "    tensor_image = tensor_image.reshape(22,22)\n",
    "    # Padding\n",
    "    tensor_image = np.pad(tensor_image, (3,3), \"constant\", constant_values=(0,0))\n",
    "    # Normalization should be done after padding i guess\n",
    "    tensor_image = (tensor_image - np.mean(trainData)) / np.std(trainData)\n",
    "    plt.imshow(tensor_image.reshape(28,28), cmap='viridis')\n",
    "    # Debugging\n",
    "#     print(tensor_image)\n",
    "#     print(np.array(compressed_output_image.getdata())) # Get data values)\n",
    "#     print(np.array(image.getdata()))\n",
    "\n",
    "    ### Compute the predictions\n",
    "    output0 = model.predict(tensor_image.reshape(1,784))\n",
    "    print(output0)\n",
    "    output = np.argmax(output0)\n",
    "    print(output)\n",
    "    \n",
    "\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 1), (event.y - 1)\n",
    "    x2, y2 = (event.x + 1), (event.y + 1)\n",
    "#     canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=24)\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\",width=12)\n",
    "    draw.line([x1, y1, x2, y2],fill=\"white\",width=4)\n",
    "\n",
    "master = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(master, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# create an empty PIL image and draw object to draw on\n",
    "output_image = PIL.Image.new(\"L\", (width, height), 0)\n",
    "draw = ImageDraw.Draw(output_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"save\",command=save)\n",
    "button.pack()\n",
    "\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3b1da",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "7, 5 and 1 are problematic still\n",
    "\n",
    "7 gets confused with 3 and 8 a lot of times for some reason.\n",
    "\n",
    "Prediction of 6, 1, 3 and 9 are much improved than the standard dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a569eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
