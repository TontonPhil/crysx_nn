{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb703b4",
   "metadata": {},
   "source": [
    "# This is a demo of how load MNIST raw pngs manually without torchvision and use torch for neural network machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from crysx_nn import mnist_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7957136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mnist_orig_png'\n",
    "trainData, trainLabels = mu.loadMNIST_plus(path_main=path, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b75697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (60000, 28, 28)\n",
      "Training labels shape (60000, 1)\n",
      "Size of training data in memory (GB) 0.3504753112792969\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape', trainData.shape)\n",
    "print('Training labels shape',trainLabels.shape)\n",
    "print('Size of training data in memory (GB)', trainData.nbytes/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0065b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "255.0\n",
      "33.318421449829934\n",
      "78.56748998339803\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 255.\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(trainData.std()) # Expected for MNIST_orig: 78.567489983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637000b",
   "metadata": {},
   "source": [
    "## Normalize within the range [0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9df9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.13066047627384325\n",
      "0.308107803856463\n"
     ]
    }
   ],
   "source": [
    "trainData = trainData/255 # Normalize\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(trainData.std()) # Expected for MNIST_orig: 0.3081078038564622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed0fd",
   "metadata": {},
   "source": [
    "## Standardize the data so that it has mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015a9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.42407389439156673\n",
      "2.8215433456893306\n",
      "-1.3299296660161542e-15\n",
      "0.9999999999999987\n"
     ]
    }
   ],
   "source": [
    "trainData = (trainData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(trainData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.std()) # Expected for MNIST_orig: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1b0af",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1796640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.]\n",
      " [4.]\n",
      " [3.]\n",
      " ...\n",
      " [5.]\n",
      " [2.]\n",
      " [0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels)\n",
    "trainLabels = mu.one_hot_encode(trainLabels, 10)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c013c",
   "metadata": {},
   "source": [
    "## Convert numpy arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70109dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainData_torch = torch.Tensor(trainData).float()\n",
    "trainLabels_torch = torch.Tensor(trainLabels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f82965",
   "metadata": {},
   "source": [
    "## Let us train a NN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756390d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n",
      "203530 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "### Define the dense neuron layer\n",
    "# Network = torch.nn.Sequential(\n",
    "#     torch.nn.Flatten(),            # 28x28 -> 784\n",
    "#     torch.nn.Linear(784, 10),      # 784 -> 10\n",
    "#     torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "# crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3251)\n"
     ]
    }
   ],
   "source": [
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = torch.zeros(1, 10, dtype=torch.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = torch.full_like(labels, 1./N)\n",
    "print(crit(output, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b167d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = trainData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11c303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb8fd4e1ae94397a529146ad94e2dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.07576535186419885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.04036523660024007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.031342953496302166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.02571651093351344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.02185868727043271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.01901976492566367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.01682396836578846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.015050925199563305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.013592754503091177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.0123634344075496\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "errorPlot = []\n",
    "\n",
    "### Train the model\n",
    "for e in tqdm(epochs):\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputsBatch = trainData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labelsBatch = trainLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        \n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputsBatch = inputsBatch.to(device=device) # move input and label tensors to the device with the model\n",
    "        labelsBatch = labelsBatch.to(device=device)\n",
    "        outputsTorch = Network(inputsBatch) # compute model outputs\n",
    "        loss = crit(outputsTorch, labelsBatch) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputsBatch) # add the batch loss to the running loss\n",
    "        samples += len(inputsBatch) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    errorPlot.append(tr_loss)\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff259",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9261ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape (10000, 28, 28)\n",
      "Test labels shape (10000, 1)\n",
      "Size of training data in memory (GB) 0.05841255187988281\n",
      "0.0\n",
      "255.0\n",
      "33.791224489795916\n",
      "79.17246322228648\n",
      "0.0\n",
      "1.0\n",
      "0.13251460584233699\n",
      "0.3104802479305348\n",
      "1.329929666016156e-15\n",
      "1.0000000000000027\n",
      "0.13251460584233862\n",
      "0.31048024793053536\n",
      "[[8.]\n",
      " [4.]\n",
      " [6.]\n",
      " ...\n",
      " [9.]\n",
      " [6.]\n",
      " [0.]]\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "path = 'mnist_orig_png'\n",
    "testData, testLabels = mu.loadMNIST_plus(path_main=path, train=False, shuffle=True)\n",
    "\n",
    "print('Test data shape', testData.shape)\n",
    "print('Test labels shape',testLabels.shape)\n",
    "print('Size of training data in memory (GB)', testData.nbytes/1024/1024/1024)\n",
    "\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 255.\n",
    "print(testData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(testData.std()) # Expected for MNIST_orig: 78.567489983\n",
    "\n",
    "## Normalize within the range [0,1.0]\n",
    "\n",
    "testData = testData/255 # Normalize\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(testData.std()) # Expected for MNIST_orig: 0.3081078038564622 \n",
    "\n",
    "## Standardize the data so that it has mean 0 and variance 1\n",
    "# Use the mean and std of training data **********\n",
    "testData = (testData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(testData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.std()) # Expected for MNIST_orig: 1.0000\n",
    "\n",
    "## Convert labels to one-hot vectors\n",
    "print(testLabels)\n",
    "testLabels = mu.one_hot_encode(testLabels, 10)\n",
    "print(testLabels)\n",
    "\n",
    "## Convert numpy arrays to torch tensors\n",
    "testData_torch = torch.Tensor(testData).float()\n",
    "testLabels_torch = torch.Tensor(testLabels).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abe86",
   "metadata": {},
   "source": [
    "## Performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d9c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = testData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e291aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.104028, accuracy: 0.964800\n"
     ]
    }
   ],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "     ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputs = testData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labels = testLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb79a7",
   "metadata": {},
   "source": [
    "## Interactive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "974f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "0\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0, 1, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "white = (255, 255, 255) # canvas back\n",
    "\n",
    "def save():\n",
    "    # save image to hard drive\n",
    "    filename = \"user_input.jpg\"\n",
    "    global output_image \n",
    "    output_image.save(filename)\n",
    "    ###### Centering begin\n",
    "    # Load image as grayscale and obtain bounding box coordinates\n",
    "    image = cv2.imread('user_input.jpg', 0)\n",
    "#     print(image)\n",
    "    height, width = image.shape\n",
    "    x,y,w,h = cv2.boundingRect(image)\n",
    "\n",
    "    # Create new blank image and shift ROI to new coordinates\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    mask = np.zeros([ROI.shape[0]+10,ROI.shape[1]+10])\n",
    "    width, height = mask.shape\n",
    "#     print(ROI.shape)\n",
    "#     print(mask.shape)\n",
    "    x = width//2 - ROI.shape[0]//2 \n",
    "    y = height//2 - ROI.shape[1]//2 \n",
    "#     print(x,y)\n",
    "    mask[y:y+h, x:x+w] = ROI\n",
    "#     print(mask)\n",
    "    # Check if centering/masking was successful\n",
    "#     plt.imshow(mask, cmap='viridis') \n",
    "    output_image = PIL.Image.fromarray(mask)\n",
    "    compressed_output_image = output_image.resize((22,22))\n",
    "#     # Enhance Saturation\n",
    "#     converter = PIL.ImageEnhance.Color(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(2.5)\n",
    "    # Enhance contrast\n",
    "#     converter = PIL.ImageEnhance.Contrast(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(3.5)\n",
    "    \n",
    "    convert_tensor = torchvision.transforms.ToTensor()\n",
    "    tensor_image = convert_tensor(compressed_output_image)\n",
    "    tensor_image = torch.nn.functional.pad(tensor_image, (3,3,3,3), \"constant\", 0)\n",
    "    # Normalization shoudl be done after padding i guess\n",
    "    convert_tensor = torchvision.transforms.Normalize((0.1307), (0.3081))\n",
    "    tensor_image = convert_tensor(tensor_image)\n",
    "    plt.imshow(tensor_image.detach().cpu().numpy().reshape(28,28), cmap='viridis')\n",
    "    # Debugging\n",
    "#     print(tensor_image)\n",
    "#     print(np.array(compressed_output_image.getdata())) # Get data values)\n",
    "#     print(np.array(image.getdata()))\n",
    "\n",
    "    ### Compute the predictions\n",
    "    with torch.no_grad():\n",
    "        output0 = Network(torch.unsqueeze(tensor_image, dim=0).to(device=device))\n",
    "        print(output0)\n",
    "        certainty, output = torch.max(output0[0], 0)\n",
    "        certainty = certainty.clone().cpu().item()\n",
    "        output = output.clone().cpu().item()\n",
    "        certainty1, output1 = torch.topk(output0[0],3)\n",
    "        certainty1 = certainty1.clone().cpu()#.item()\n",
    "        output1 = output1.clone().cpu()#.item()\n",
    "#     print(certainty)\n",
    "    print(output)\n",
    "        \n",
    "    print(certainty1)\n",
    "    print(output1)\n",
    "\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 1), (event.y - 1)\n",
    "    x2, y2 = (event.x + 1), (event.y + 1)\n",
    "#     canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=24)\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\",width=12)\n",
    "    draw.line([x1, y1, x2, y2],fill=\"white\",width=4)\n",
    "\n",
    "master = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(master, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# create an empty PIL image and draw object to draw on\n",
    "output_image = PIL.Image.new(\"L\", (width, height), 0)\n",
    "draw = ImageDraw.Draw(output_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"save\",command=save)\n",
    "button.pack()\n",
    "\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd01cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
