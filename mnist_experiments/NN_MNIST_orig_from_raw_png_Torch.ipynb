{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb703b4",
   "metadata": {},
   "source": [
    "# This is a demo of how load to MNIST raw pngs manually without torchvision and use torch for neural network machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12234e8c",
   "metadata": {},
   "source": [
    "## Run the following for Google colab \n",
    "then restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d3c2163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting IPython==7.7.0\n",
      "  Using cached ipython-7.7.0-py3-none-any.whl (774 kB)\n",
      "Requirement already satisfied: pygments in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (2.10.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from IPython==7.7.0) (58.0.4)\n",
      "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
      "  Using cached prompt_toolkit-2.0.10-py3-none-any.whl (340 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from jedi>=0.10->IPython==7.7.0) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython==7.7.0) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\manas\\anaconda3\\envs\\crysx_nn\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython==7.7.0) (1.16.0)\n",
      "Installing collected packages: prompt-toolkit, IPython\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.20\n",
      "    Uninstalling prompt-toolkit-3.0.20:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.20\n",
      "  Attempting uninstall: IPython\n",
      "    Found existing installation: ipython 7.29.0\n",
      "    Uninstalling ipython-7.29.0:\n",
      "      Successfully uninstalled ipython-7.29.0\n",
      "Successfully installed IPython-7.7.0 prompt-toolkit-2.0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipykernel 6.4.1 requires ipython<8.0,>=7.23.1, but you have ipython 7.7.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\manas\\appdata\\roaming\\python\\python37\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --no-cache-dir https://github.com/manassharma07/crysx_nn/tarball/main\n",
    "! pip install IPython==7.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from crysx_nn import mnist_utils as mu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c595e3",
   "metadata": {},
   "source": [
    "## Download MNIST_orig and MNIST_orig dataset (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffade76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mu.downloadMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ff0b8",
   "metadata": {},
   "source": [
    "## Load the training dataset from MNIST_orig in memory (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7801454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = 'MNIST-PLUS-PNG/mnist_orig_png'\n",
    "trainData, trainLabels = mu.loadMNIST(path_main=path, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b75697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (60000, 28, 28)\n",
      "Training labels shape (60000, 1)\n",
      "Size of training data in memory (GB) 0.3504753112792969\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape', trainData.shape)\n",
    "print('Training labels shape',trainLabels.shape)\n",
    "print('Size of training data in memory (GB)', trainData.nbytes/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0065b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "255.0\n",
      "33.318421449829934\n",
      "78.56748998339799\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 255.\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(trainData.std()) # Expected for MNIST_orig: 78.567489983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637000b",
   "metadata": {},
   "source": [
    "## Normalize within the range [0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9df9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.13066047627384314\n",
      "0.3081078038564628\n"
     ]
    }
   ],
   "source": [
    "trainData = trainData/255 # Normalize\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(trainData.std()) # Expected for MNIST_orig: 0.3081078038564622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed0fd",
   "metadata": {},
   "source": [
    "## Standardize the data so that it has mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015a9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4240738943915667\n",
      "2.8215433456893333\n",
      "-9.112299728078798e-16\n",
      "0.9999999999999992\n"
     ]
    }
   ],
   "source": [
    "trainData = (trainData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(trainData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.std()) # Expected for MNIST_orig: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1b0af",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1796640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.]\n",
      " [7.]\n",
      " [7.]\n",
      " ...\n",
      " [5.]\n",
      " [8.]\n",
      " [8.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels)\n",
    "trainLabels = mu.one_hot_encode(trainLabels, 10)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c013c",
   "metadata": {},
   "source": [
    "## Convert numpy arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70109dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainData_torch = torch.Tensor(trainData).float()\n",
    "trainLabels_torch = torch.Tensor(trainLabels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f82965",
   "metadata": {},
   "source": [
    "## Let us train a NN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756390d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n",
      "203530 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "### Define the dense neuron layer\n",
    "# Network = torch.nn.Sequential(\n",
    "#     torch.nn.Flatten(),            # 28x28 -> 784\n",
    "#     torch.nn.Linear(784, 10),      # 784 -> 10\n",
    "#     torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "# crit = torch.nn.BCELoss()\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f7041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3026)\n"
     ]
    }
   ],
   "source": [
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = torch.zeros(1, 10, dtype=torch.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = torch.full_like(labels, 1./N)\n",
    "print(crit(output, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b167d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = trainData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c11c303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e871ea45ed041c1acd06ff7e59f31b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.6184447677930196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.5363473467032114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.5204128781954447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.5108132429917653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.504015684525172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1.49883646885554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1.4947499128182729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1.4914902834097545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1.4887672527631124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.4864646061261495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1.4845313521226247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1.4828836766878764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1.4814352409044902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1.4801971626281738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1.4791448028882344\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 16)\n",
    "\n",
    "errorPlot = []\n",
    "\n",
    "### Train the model\n",
    "for e in tqdm(epochs):\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputsBatch = trainData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labelsBatch = trainLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        \n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputsBatch = inputsBatch.to(device=device) # move input and label tensors to the device with the model\n",
    "        labelsBatch = labelsBatch.to(device=device)\n",
    "        outputsTorch = Network(inputsBatch) # compute model outputs\n",
    "        loss = crit(outputsTorch, labelsBatch) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputsBatch) # add the batch loss to the running loss\n",
    "        samples += len(inputsBatch) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    errorPlot.append(tr_loss)\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff259",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9261ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape (10000, 28, 28)\n",
      "Test labels shape (10000, 1)\n",
      "Size of training data in memory (GB) 0.05841255187988281\n",
      "0.0\n",
      "255.0\n",
      "33.791224489795916\n",
      "79.17246322228651\n",
      "0.0\n",
      "1.0\n",
      "0.132514605842337\n",
      "0.3104802479305351\n",
      "9.112299728078806e-16\n",
      "1.0000000000000018\n",
      "0.1325146058423381\n",
      "0.31048024793053536\n",
      "[[6.]\n",
      " [2.]\n",
      " [8.]\n",
      " ...\n",
      " [2.]\n",
      " [6.]\n",
      " [9.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "path = 'MNIST-PLUS-PNG/mnist_orig_png'\n",
    "testData, testLabels = mu.loadMNIST(path_main=path, train=False, shuffle=True)\n",
    "\n",
    "print('Test data shape', testData.shape)\n",
    "print('Test labels shape',testLabels.shape)\n",
    "print('Size of training data in memory (GB)', testData.nbytes/1024/1024/1024)\n",
    "\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 255.\n",
    "print(testData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(testData.std()) # Expected for MNIST_orig: 78.567489983\n",
    "\n",
    "## Normalize within the range [0,1.0]\n",
    "\n",
    "testData = testData/255 # Normalize\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(testData.std()) # Expected for MNIST_orig: 0.3081078038564622 \n",
    "\n",
    "## Standardize the data so that it has mean 0 and variance 1\n",
    "# Use the mean and std of training data **********\n",
    "testData = (testData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(testData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.std()) # Expected for MNIST_orig: 1.0000\n",
    "\n",
    "## Convert labels to one-hot vectors\n",
    "print(testLabels)\n",
    "testLabels = mu.one_hot_encode(testLabels, 10)\n",
    "print(testLabels)\n",
    "\n",
    "## Convert numpy arrays to torch tensors\n",
    "testData_torch = torch.Tensor(testData).float()\n",
    "testLabels_torch = torch.Tensor(testLabels).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abe86",
   "metadata": {},
   "source": [
    "## Performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00d9c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = testData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29e291aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.593181, accuracy: 0.965600\n"
     ]
    }
   ],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "     ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputs = testData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labels = testLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb79a7",
   "metadata": {},
   "source": [
    "## Interactive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "974f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4294e-37,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "0\n",
      "tensor([1.0000e+00, 3.4294e-37, 0.0000e+00])\n",
      "tensor([0, 5, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "white = (255, 255, 255) # canvas back\n",
    "\n",
    "def save():\n",
    "    # save image to hard drive\n",
    "    filename = \"user_input.jpg\"\n",
    "    global output_image \n",
    "    output_image.save(filename)\n",
    "    ###### Centering begin\n",
    "    # Load image as grayscale and obtain bounding box coordinates\n",
    "    image = cv2.imread('user_input.jpg', 0)\n",
    "#     print(image)\n",
    "    height, width = image.shape\n",
    "    x,y,w,h = cv2.boundingRect(image)\n",
    "\n",
    "    # Create new blank image and shift ROI to new coordinates\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    mask = np.zeros([ROI.shape[0]+10,ROI.shape[1]+10])\n",
    "    width, height = mask.shape\n",
    "#     print(ROI.shape)\n",
    "#     print(mask.shape)\n",
    "    x = width//2 - ROI.shape[0]//2 \n",
    "    y = height//2 - ROI.shape[1]//2 \n",
    "#     print(x,y)\n",
    "    mask[y:y+h, x:x+w] = ROI\n",
    "#     print(mask)\n",
    "    # Check if centering/masking was successful\n",
    "#     plt.imshow(mask, cmap='viridis') \n",
    "    output_image = PIL.Image.fromarray(mask)\n",
    "    compressed_output_image = output_image.resize((22,22))\n",
    "#     # Enhance Saturation\n",
    "#     converter = PIL.ImageEnhance.Color(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(2.5)\n",
    "    # Enhance contrast\n",
    "#     converter = PIL.ImageEnhance.Contrast(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(3.5)\n",
    "    \n",
    "    convert_tensor = torchvision.transforms.ToTensor()\n",
    "    tensor_image = convert_tensor(compressed_output_image)\n",
    "    tensor_image = torch.nn.functional.pad(tensor_image, (3,3,3,3), \"constant\", 0)\n",
    "    # Normalization shoudl be done after padding i guess\n",
    "    convert_tensor = torchvision.transforms.Normalize((0.1307), (0.3081))\n",
    "    tensor_image = convert_tensor(tensor_image)\n",
    "    plt.imshow(tensor_image.detach().cpu().numpy().reshape(28,28), cmap='viridis')\n",
    "    # Debugging\n",
    "#     print(tensor_image)\n",
    "#     print(np.array(compressed_output_image.getdata())) # Get data values)\n",
    "#     print(np.array(image.getdata()))\n",
    "\n",
    "    ### Compute the predictions\n",
    "    with torch.no_grad():\n",
    "        output0 = Network(torch.unsqueeze(tensor_image, dim=0).to(device=device))\n",
    "        print(output0)\n",
    "        certainty, output = torch.max(output0[0], 0)\n",
    "        certainty = certainty.clone().cpu().item()\n",
    "        output = output.clone().cpu().item()\n",
    "        certainty1, output1 = torch.topk(output0[0],3)\n",
    "        certainty1 = certainty1.clone().cpu()#.item()\n",
    "        output1 = output1.clone().cpu()#.item()\n",
    "#     print(certainty)\n",
    "    print(output)\n",
    "        \n",
    "    print(certainty1)\n",
    "    print(output1)\n",
    "\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 1), (event.y - 1)\n",
    "    x2, y2 = (event.x + 1), (event.y + 1)\n",
    "#     canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=24)\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\",width=12)\n",
    "    draw.line([x1, y1, x2, y2],fill=\"white\",width=4)\n",
    "\n",
    "master = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(master, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# create an empty PIL image and draw object to draw on\n",
    "output_image = PIL.Image.new(\"L\", (width, height), 0)\n",
    "draw = ImageDraw.Draw(output_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"save\",command=save)\n",
    "button.pack()\n",
    "\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9093448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
